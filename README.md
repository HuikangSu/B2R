# Boundary-to-Region Supervision for Offline Safe Reinforcement Learning (NIPS2025)

This repository provides the official implementation of **B2R**. It is evaluated on the [DSRL benchmark](https://github.com/decisionintelligence/DSRL) across SafetyGymnasium, BulletSafetyGym, and MetaDrive environments.

Problem ‚Üí Existing sequence models (Decision Transformer‚Äìstyle) treat return-to-go (RTG) and cost-to-go (CTG) symmetrically. But in constrained MDPs, RTG is a flexible target; CTG is a non-negotiable safety budget. This ‚Äúsymmetry fallacy‚Äù causes:
- Brittle deployment: picking a feasible RTG/CTG pair is guesswork.
- Sparse training signal: few trajectories sit near the safety boundary, so supervision is thin and unreliable.

Insight ‚Üí Safety is a boundary, not a knob. If the model always sees the same boundary token (the deployment budget), it can learn all the diverse safe behaviors inside that region, not just rare near-threshold cases.

Solution (B2R): Boundary-to-Region supervision that makes conditioning asymmetric by realigning CTG to the budget. Three steps: filter, realign, encode. The result is dense, region-wide safety supervision without changing the Transformer objective or architecture.

![Figure 0](./figure_1_score0.97.jpg)

How B2R works in practice:
1) Trajectory filtering: drop unsafe trajectories whose total cost exceeds the deployment budget Œ∫.
2) CTG realignment: shift each safe trajectory‚Äôs cost-to-go so CTG starts at Œ∫ while preserving its strictly decreasing temporal profile:
   C‚Ä≤t = Ct + (Œ∫ ‚àí C(œÑ)).
3) Temporal encoding with RoPE: rotary positional embeddings capture relative, step-by-step dynamics better than absolute positions.

![Figure 1](./figure_5_score0.95.jpg)

Why it works (intuition):
- The model is always conditioned on a fixed boundary token (CTG = Œ∫). RTG stays a user-controlled target.
- Training sees the same boundary across many safe behaviors ‚Üí supervision becomes dense and consistent.
- At inference, RTG and CTG update by subtracting observed rewards/costs each step, anchoring safety while tuning ambition.

Grounding in theory:
- With aligned data and bounded per-step cost error œÉ, if œÉH < Œ¥ (H = horizon, Œ¥ = safety margin), B2R achieves:
  ‚Ä¢ High-probability safety: violation probability shrinks exponentially in (Œ¥ ‚àí œÉH)^2 / (2H C_max^2).
  ‚Ä¢ Expected safety: E[cumulative cost] ‚â§ Œ∫ ‚àí (Œ¥ ‚àí œÉH).
- With ‚Äúoptimal coverage‚Äù in the filtered data, region-wide supervision contains boundary-only supervision, so reward is no worse while constraints are satisfied.

Hard data across 38 tasks (Safety Gymnasium, Bullet Safety-Gym, MetaDrive; 3 cost limits √ó 3 seeds):
- B2R satisfies safety in 35/38 and ranks first on average.
- Highest rewards in 20 tasks.
- Consistently lower cost than Constrained Decision Transformer (CDT) while matching or beating reward. Tightening CDT‚Äôs boundary token isn‚Äôt enough‚Äîwhat matters is region-wide supervision.

Robustness and flexibility:
- Safe data scarcity (5‚Äì50% of safe trajectories): B2R degrades gracefully; often retains safety at 20% due to dense supervision.
- Multi-target extension: one model supports multiple Œ∫ values via per-Œ∫ realignment and conditioning‚Äîcomparable to single-target models.

Takeaway: Stop treating safety like reward. Align cost signals to the actual budget and supervise across the whole safe region. B2R keeps DT‚Äôs simplicity, adds safety guarantees, and improves the safety‚Äìperformance frontier‚Äîno architectural surgery required.

## üöÄ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/B2R-ULTRA.git
cd B2R-ULTRA

# Install the package
pip install -e .
```

### Training

```bash
python main/B2R_main.py --env CarButton1 --cost_limit 20 --seed 0
```

## üôè Acknowledgements

- [DSRL](https://github.com/decisionintelligence/DSRL) ‚Äì Benchmark for offline safe RL
- [Decision Transformer](https://github.com/kzl/decision-transformer) ‚Äì Transformer-based RL modeling

## üìÑ Citation

```bibtex
@inproceedings{b2r2025,
  title={Boundary-to-Region Supervision for Offline Safe Reinforcement Learning},
  author={Huikang Su, Dengyun Peng, Zifeng Zhuang, YuHan Liu, Qiguang Chen, Donglin Wang, Qinghe Liu},
  booktitle={NeurIPS},
  year={2025}
}
```

## üõ† License
This project is licensed under the MIT License.
